================================================================================
GPU-ENABLED KUBERNETES AUTOSCALER - STEP-BY-STEP EXECUTION GUIDE
================================================================================

IMPORTANT: Run each command and verify output before proceeding to next step.
Copy-paste each command into your terminal.

================================================================================
PHASE 1: SYSTEM VERIFICATION
================================================================================

[1.1] Check Disk Space
----------------------
df -h / /home

Expected: At least 10GB free on root filesystem


[1.2] Verify GPU
----------------
nvidia-smi

Expected: Shows NVIDIA GeForce GTX 1050 Ti, Driver 580.95.05


[1.3] Check Docker
------------------
docker --version
docker ps

Expected: Docker version 20.10+ or newer, no errors


[1.4] Check Minikube
--------------------
minikube version

Expected: minikube version v1.34.0 or newer


[1.5] Check kubectl
-------------------
kubectl version --client

Expected: kubectl version 1.28+ or newer


[1.6] Check Python
------------------
python3 --version
pip3 --version

Expected: Python 3.8+ and pip3


================================================================================
PHASE 2: NVIDIA CONTAINER TOOLKIT TEST
================================================================================

[2.1] Test Docker GPU Access
-----------------------------
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

Expected: Shows GPU info inside container (same as host nvidia-smi)

If this FAILS, you need to install NVIDIA Container Toolkit:
    distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
    curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
    sudo apt-get update
    sudo apt-get install -y nvidia-container-toolkit
    sudo systemctl restart docker


[2.2] Verify NVIDIA Runtime
----------------------------
docker info | grep -i nvidia

Expected: Shows nvidia runtime configuration


================================================================================
PHASE 3: SETUP GLOBAL PYTHON VIRTUAL ENVIRONMENT
================================================================================

[3.1] Create Virtual Environment at /usr/local/pyenv
-----------------------------------------------------
sudo python3 -m venv /usr/local/pyenv


[3.2] Change Ownership
----------------------
sudo chown -R $USER:$USER /usr/local/pyenv


[3.3] Activate Virtual Environment
-----------------------------------
source /usr/local/pyenv/bin/activate

Your prompt should now show (pyenv) prefix


[3.4] Upgrade pip
-----------------
pip install --upgrade pip


[3.5] Install Python Dependencies
----------------------------------
pip install nvidia-ml-py3 cupy-cuda12x fastapi uvicorn numpy requests psutil pydantic kubernetes httpx tenacity

This will take 2-5 minutes (CuPy is large ~500MB)


[3.6] Verify Installations
---------------------------
pip list | grep -E "nvidia-ml-py3|cupy"

Expected:
    cupy-cuda12x    12.3.0 (or similar)
    nvidia-ml-py3   7.352.0 (or similar)


================================================================================
PHASE 4: VERIFY PROJECT FILES
================================================================================

[4.1] Navigate to Project
--------------------------
cd "/home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2"


[4.2] List Files
----------------
ls -lah

Expected: See setup.py, demo.py, monitor.py, verify_gpu.py, Dockerfile.gpu


[4.3] Check Subdirectories
---------------------------
ls -lah app/
ls -lah k8s/
ls -lah scaler/

Expected:
    app/ → unified_app.py, requirements.txt
    k8s/ → userscale-gpu.yaml, hpa-gpu.yaml
    scaler/ → main.py, requirements.txt


================================================================================
PHASE 5: RUN GPU VERIFICATION
================================================================================

[5.1] Execute GPU Verification Script
--------------------------------------
python3 verify_gpu.py

Expected Output:
    ✅ nvidia-smi - PASS
    ✅ pynvml - PASS
    ✅ cupy - PASS
    ✅ docker-gpu - PASS

If any FAIL, fix the issue before proceeding.


================================================================================
PHASE 6: DEPLOY TO KUBERNETES
================================================================================

[6.1] Clean Existing Cluster (if any)
--------------------------------------
minikube delete

This removes any old cluster


[6.2] Run Setup Script
----------------------
python3 setup.py

This will:
    - Start Minikube with GPU support
    - Build Docker image (takes 5-10 minutes)
    - Deploy to Kubernetes
    - Setup port forwarding

Expected: "Setup complete!" message at the end


[6.3] Verify Pods are Running
------------------------------
kubectl get pods -n userscale
kubectl get pods -n hpa

Expected: All pods in "Running" state (may take 1-2 minutes)


[6.4] Wait for Pods to be Ready
--------------------------------
kubectl wait --for=condition=ready pod -l app=userscale-app -n userscale --timeout=300s
kubectl wait --for=condition=ready pod -l app=hpa-app -n hpa --timeout=300s


[6.5] Check Services
--------------------
kubectl get svc -n userscale
kubectl get svc -n hpa


[6.6] Test Endpoints
--------------------
curl http://localhost:8001/healthz
curl http://localhost:8002/healthz

Expected: {"status": "healthy"}


[6.7] Check GPU in Cluster
---------------------------
kubectl describe node | grep -A 5 "nvidia.com/gpu"

Expected: Shows GPU capacity and allocatable resources


================================================================================
PHASE 7: OPEN MONITORING TERMINALS
================================================================================

Open 3 separate terminals for real-time monitoring:

[Terminal 1] Watch GPU Utilization
-----------------------------------
watch -n 1 nvidia-smi


[Terminal 2] Watch Pod Scaling
-------------------------------
watch -n 1 "kubectl get pods -n userscale && echo && kubectl get pods -n hpa"


[Terminal 3] Run Monitor Script
--------------------------------
cd "/home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2"
source /usr/local/pyenv/bin/activate
python3 monitor.py


================================================================================
PHASE 8: RUN DEMO WORKLOAD
================================================================================

[8.1] In Main Terminal, Run Demo
---------------------------------
cd "/home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2"
source /usr/local/pyenv/bin/activate
python3 demo.py

This will:
    - Run for 5 minutes (300 seconds)
    - Generate GPU workload
    - Track scaling behavior
    - Show final comparison report

Watch the other terminals to see:
    - GPU utilization increase (Terminal 1)
    - Pods scaling up (Terminal 2)
    - Real-time metrics (Terminal 3)


================================================================================
PHASE 9: VALIDATION
================================================================================

[9.1] Check Final GPU State
----------------------------
nvidia-smi


[9.2] Check Final Pod Count
----------------------------
kubectl get pods -n userscale
kubectl get pods -n hpa


[9.3] Check HPA Status
----------------------
kubectl get hpa -n hpa -o wide


[9.4] Check Scaler Logs
-----------------------
kubectl logs -n userscale -l app=userscale-scaler --tail=100


[9.5] Check App Metrics
-----------------------
curl http://localhost:8001/metrics | jq .

Expected: Real GPU metrics (no "simulated" flags)


[9.6] Verify Real GPU Metrics
------------------------------
curl http://localhost:8001/metrics | grep -i gpu

Expected: gpu_utilization, gpu_memory_used, gpu_temperature


================================================================================
PHASE 10: CLEANUP
================================================================================

[10.1] Stop Demo and Monitor
-----------------------------
Press Ctrl+C in demo.py and monitor.py terminals


[10.2] Run Cleanup Script
--------------------------
python3 setup.py --cleanup

This will:
    - Delete Kubernetes resources
    - Stop port forwarding
    - Clean up namespaces


[10.3] Stop Minikube (Optional)
--------------------------------
minikube stop


[10.4] Delete Cluster (Optional)
---------------------------------
minikube delete


================================================================================
TROUBLESHOOTING COMMANDS
================================================================================

If pods are stuck in Pending:
------------------------------
kubectl describe pod <pod-name> -n userscale
kubectl get events -n userscale --sort-by='.lastTimestamp'


If GPU not detected in pods:
-----------------------------
kubectl get pods -n kube-system | grep nvidia
kubectl logs -n kube-system -l name=nvidia-device-plugin-ds


If port forwarding fails:
--------------------------
pkill -f 'kubectl port-forward'
kubectl port-forward -n userscale svc/userscale-app 8001:8000 &
kubectl port-forward -n hpa svc/hpa-app 8002:8000 &


If Docker build fails:
----------------------
docker system prune -af
docker images
docker rmi userscale-gpu:latest


Check Minikube status:
----------------------
minikube status
minikube logs


================================================================================
EXPECTED RESULTS
================================================================================

✅ GPU utilization should reach 60-90% during demo
✅ UserScale should scale faster than HPA (GPU-aware vs CPU-based)
✅ Pods should scale from 1 to 5-10 replicas
✅ All metrics should show "real" values (no simulation)
✅ Temperature should increase to 60-75°C under load
✅ Memory usage should increase to 500-1500 MB

================================================================================
END OF STEP-BY-STEP GUIDE
================================================================================

For questions, refer to:
    - PROJECT_SUMMARY.txt
    - EXECUTION_GUIDE.md
    - verify_gpu.py output
