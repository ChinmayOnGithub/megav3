================================================================================
                    COMMAND SUMMARY - READY TO EXECUTE
================================================================================

PROJECT: GPU-Enabled Kubernetes Autoscaler
LOCATION: /home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2
GPU: NVIDIA GeForce GTX 1050 Ti (4GB)
SYSTEM: Ubuntu 22.04 Native

================================================================================
                        DOCUMENTATION CREATED
================================================================================

✅ START_HERE.md                 - Quick overview and getting started
✅ EXECUTION_CHECKLIST.md         - Complete checklist with checkboxes  
✅ STEP_BY_STEP_COMMANDS.txt      - Detailed commands with explanations
✅ QUICK_COMMANDS.txt             - Essential commands reference
✅ EXECUTION_COMMANDS.sh          - Full bash script
✅ COMMAND_SUMMARY.txt            - This file

================================================================================
                    PHASE 1: SYSTEM VERIFICATION
================================================================================

cd "/home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2"

nvidia-smi
docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi
docker --version
minikube version
kubectl version --client
python3 --version

Expected: All commands succeed, GPU visible in Docker

================================================================================
                    PHASE 2: PYTHON ENVIRONMENT SETUP
================================================================================

sudo python3 -m venv /usr/local/pyenv
sudo chown -R $USER:$USER /usr/local/pyenv
source /usr/local/pyenv/bin/activate
pip install --upgrade pip
pip install nvidia-ml-py3 cupy-cuda12x fastapi uvicorn numpy requests psutil pydantic kubernetes httpx tenacity

Expected: All packages install successfully (takes 5-10 minutes)

================================================================================
                    PHASE 3: GPU VERIFICATION
================================================================================

python3 verify_gpu.py

Expected Output:
    ✅ nvidia-smi - PASS
    ✅ pynvml - PASS
    ✅ cupy - PASS
    ✅ docker-gpu - PASS

If any FAIL, stop and fix before continuing!

================================================================================
                    PHASE 4: KUBERNETES DEPLOYMENT
================================================================================

python3 setup.py

Expected: 
    - Minikube starts with GPU support
    - Docker image builds (10-15 minutes first time)
    - Pods deploy successfully
    - Port forwarding starts on 8001, 8002

Verify:
    kubectl get pods -n userscale
    kubectl get pods -n hpa
    curl http://localhost:8001/healthz
    curl http://localhost:8002/healthz

================================================================================
                    PHASE 5: MONITORING SETUP
================================================================================

Open 3 separate terminals:

Terminal 1 (GPU):
    watch -n 1 nvidia-smi

Terminal 2 (Pods):
    watch -n 1 "kubectl get pods -n userscale && echo && kubectl get pods -n hpa"

Terminal 3 (Dashboard):
    cd "/home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2"
    source /usr/local/pyenv/bin/activate
    python3 monitor.py

================================================================================
                    PHASE 6: RUN DEMO WORKLOAD
================================================================================

In main terminal:
    python3 demo.py

Expected:
    - Runs for 5 minutes (300 seconds)
    - GPU utilization increases to 60-90%
    - Pods scale from 1 to 5-10 replicas
    - UserScale scales faster than HPA
    - Final report shows comparison

Watch in monitoring terminals:
    - Terminal 1: GPU % increases
    - Terminal 2: Pod count increases
    - Terminal 3: Real-time metrics update

================================================================================
                    PHASE 7: VALIDATION
================================================================================

nvidia-smi
kubectl get pods -n userscale
kubectl get pods -n hpa
kubectl get hpa -n hpa -o wide
kubectl logs -n userscale -l app=userscale-scaler --tail=50
curl http://localhost:8001/metrics | jq .

Expected:
    - GPU utilization elevated
    - Multiple replicas running
    - Real GPU metrics (no "simulated")
    - Scaler logs show GPU-based decisions

================================================================================
                    PHASE 8: CLEANUP
================================================================================

python3 setup.py --cleanup
minikube stop
minikube delete

Expected: All resources cleaned up

================================================================================
                        TROUBLESHOOTING
================================================================================

Docker GPU fails:
    sudo apt-get install -y nvidia-container-toolkit
    sudo systemctl restart docker

Pods stuck pending:
    kubectl describe pod <pod-name> -n userscale
    kubectl get events -n userscale

Port forwarding fails:
    pkill -f 'kubectl port-forward'

Minikube issues:
    minikube delete
    minikube start --driver=docker --gpus=all

================================================================================
                        TIME ESTIMATES
================================================================================

Phase 1: System Verification         →  2 minutes
Phase 2: Python Environment Setup     → 10 minutes
Phase 3: GPU Verification             →  2 minutes
Phase 4: Kubernetes Deployment        → 15 minutes (first time)
Phase 5: Monitoring Setup             →  2 minutes
Phase 6: Run Demo Workload            →  5 minutes
Phase 7: Validation                   →  3 minutes
Phase 8: Cleanup                      →  2 minutes

TOTAL TIME: ~40 minutes (first run)
SUBSEQUENT RUNS: ~10 minutes (image already built)

================================================================================
                        SUCCESS CRITERIA
================================================================================

✅ verify_gpu.py shows all PASS
✅ setup.py completes without errors
✅ All pods reach Running state
✅ Health checks return {"status": "healthy"}
✅ GPU utilization reaches 60-90% during demo
✅ Pods scale from 1 to 5-10 replicas
✅ UserScale scales faster than HPA
✅ All metrics show real values (no "simulated")
✅ Temperature increases to 60-75°C
✅ demo.py completes with comparison report

================================================================================
                        NEXT STEPS
================================================================================

1. Open START_HERE.md for quick overview
2. Open EXECUTION_CHECKLIST.md for detailed step-by-step
3. Keep QUICK_COMMANDS.txt open for reference
4. Execute commands one by one
5. Verify each phase before proceeding

================================================================================
                        READY TO BEGIN!
================================================================================

Start with:
    cd "/home/chinmay/Development/GitHub Repos/Mega_Project_SEMVII_version2"
    cat START_HERE.md

Or jump straight to:
    nvidia-smi
    docker run --rm --gpus all nvidia/cuda:12.2.0-base-ubuntu22.04 nvidia-smi

================================================================================
